#summary One-sentence summary of this page.

= Introduction =

There are many researches that require an appropriate Resource Description Framework (RDF) dataset for their experiments. However, we still lack such a good dataset because:
 # Some existing datasets do not contain large numbers of triplets, resources, or predicates, and so the scalability of real world data (e.g. Wikipedia) cannot be reflected.
 # Some datasets are static (which is not suitable for testing database update approaches)
 # Some datasets require specific domain knowledge

Wikipedia is a good data source because of its large scale and is always changing as it is collaboratively contributed by authors. Moreover, most of its contents can be understood by general readers. Wikipedia is frequently backed up into data dumps in XML format available for download. In our project we try to utilize the structured infobox data by converting them into RDF triplets.

We aim at providing an extraction toolkit which enables users to generate Wikipedia infobox RDF data from the Wikipedia XML data dumps.

= Brief Description of the Toolkit =
This toolkit extracts the infobox RDF data from the Wikipedia data dumps in XML format, and the extracted data are stored in a user-specified PostgreSQL database. It consists of an infobox data extraction tool, a data cleansing tool, and the RDF conversion tool, and they are used in the following three different phases.

== Infobox Data Extraction ==
Besides the infobox contents, the Wikipedia data dump also contains other data such as page paragraphs. In order to speed up the performance for subsequent cleansing and extraction processes, it is better to extract essential contents only into an intermediate XML file.

== Data Cleansing ==
This process removes the WikiMedia markups, comments and HTML tags from the infobox data. It also performs infobox redirection, which replaces infobox alias names by their canonical names, and so semantically identical infoboxes can be grouped together.

== RDF Conversion ==
The cleansed infobox data are transformed into an RDF dataset according to the following logic:
 * The subject is the Wikipedia page name (e.g. Tsing_Ma_Bridge)
 * The predicate is the concatenation of the infobox name, the "#" symbol and the infobox property name (e.g. Bridge#bridge_name)
 * The object is extracted from the property value, and a property value may produce one or more objects, thus several RDF triplets may be generated

*It is recommended to refer to the technical report for more technical details of the above processes.*

= Usage Guide =
 # Download the data dump from the download site [http://dumps.wikimedia.org/enwiki/] (English Wikipedia, you may select other languages as well).